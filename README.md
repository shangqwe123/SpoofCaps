## Spoof-Caps: Multi-Modal LLM Driven Speech Spoof Captions

With the rapid advancement of speech spoofing technologies—such as voice cloning, speech synthesis, and voice conversion—the detection of spoofed speech demands more efficient and explainable solutions. Current detection methods typically focus on binary classification (e.g., labeling speech as "spoofed" or "real"), which often lacks interpretability regarding specific spoofing characteristics or generation methodologies.

To address this gap, this paper proposes **Spoof-Caps**, a novel framework that integrates multi-modal large language models (LLMs) to generate descriptive captions for spoofing detection. Our approach aims to provide fine-grained explanations of spoofed speech, including:

- Identifying spoofing types.
- Inferring potential generation techniques.
- Quantifying spoofing severity through multi-modal analysis.

To enable these capabilities, we have constructed a novel **Spoof-Caption dataset** by augmenting standard spoofing corpora with multi-level textual annotations.
