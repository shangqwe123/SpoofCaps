With the rapid advancement of speech spoofing technologies—including voice cloning, speech synthesis, and voice conversion—the detection of spoofed speech demands more efficient and explainable solutions. Current detection methods typically focus on binary classification (e.g., labeling speech as "spoofed" or "real"), which often lacks interpretability regarding specific spoofing characteristics or generation methodologies. To address this gap, this paper proposes \textbf{Spoof-Caps}, a novel framework that attempts to integrate multi-modal large language models (LLMs) to generate descriptive captions for spoofing detection. Our approach aims to provide fine-grained explanations of spoofed speech, such as identifying spoofing types, inferring potential generation techniques, and quantifying spoofing severity through multi-modal analysis. To enable this capability, we construct a novel \textbf{Spoof-Caption dataset} by augmenting standard spoofing corpora with multi-level textual annotations.
